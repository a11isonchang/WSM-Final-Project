import json
import os
from openai import OpenAI
from tqdm import tqdm

# --- è¨­å®šå€åŸŸ ---
INPUT_FILE = './dragonball_dataset/queries_show/test_queries_zh.jsonl'
OUTPUT_FILE = './database/database_test.jsonl'

# å¡«å…¥ä½ çš„ OpenRouter API Key
OPENROUTER_API_KEY = "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" 

# åœ¨é€™è£¡å¡«å…¥ä½ æƒ³ç”¨çš„ OpenRouter æ¨¡å‹åç¨±
# ä¾‹å¦‚ xAI çš„ Grok (å‡è¨­æ˜¯ grok-2 æˆ–å…¶ä»–): "x-ai/grok-2-1212"
# æˆ–æ˜¯ Llama 3.3 70B: "meta-llama/llama-3.3-70b-instruct"
# æˆ–æ˜¯ Gemini 2.0 Flash (è¶…å¿«): "google/gemini-2.0-flash-exp:free"
MODEL_NAME = "x-ai/grok-4.1-fast"  # è«‹ç¢ºèª OpenRouter ä¸Šçš„ç¢ºåˆ‡ ID

def extract_keywords_with_openrouter(client, query_text):
    """
    ä½¿ç”¨ OpenRouter API æå–é—œéµå­— (JSON Mode)
    """
    system_prompt = """
    You are an expert medical data analyst. 
    Extract key search terms from the query.
    Output purely in JSON format with a single key "keywords".
    Example: {"keywords": ["Hospital A", "Disease B", "2024"]}
    """

    user_prompt = f"Extract keywords from: {query_text}"

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            # è¨±å¤šå•†æ¥­æ¨¡å‹æ”¯æ´ response_format={"type": "json_object"}ï¼Œé€™èƒ½ä¿è­‰ JSON æ ¼å¼
            # å¦‚æœ Grok æš«æ™‚ä¸æ”¯æ´æ­¤åƒæ•¸ï¼Œå¯ä»¥æ‹¿æ‰é€™è¡Œï¼Œä½†é€šå¸¸ Prompt å¤ å¼·å°±æ²’å•é¡Œ
            response_format={"type": "json_object"}, 
            temperature=0.1, # é™ä½éš¨æ©Ÿæ€§ï¼Œè¶Šä½è¶Šæº–
        )
        
        raw_output = response.choices[0].message.content.strip()
        
        # è§£æ JSON
        json_res = json.loads(raw_output)
        keywords = json_res.get("keywords", [])
        
        return keywords
    except Exception as e:
        print(f"\n[Error] {e}")
        # å¦‚æœæ˜¯ JSON è§£æå¤±æ•—ï¼Œå˜—è©¦ç°¡å–®çš„å­—ä¸²è™•ç†è£œæ•‘
        return []

def process_dataset():
    # åˆå§‹åŒ– OpenAI Clientï¼Œä½†æŒ‡å‘ OpenRouter
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
        # OpenRouter å»ºè­°åŠ é€™å…©å€‹ header ä»¥ä¾¿ä»–å€‘çµ±è¨ˆæ’å
        default_headers={
            "HTTP-Referer": "https://github.com/YourProject", 
            "X-Title": "WSM RAG Preprocessing" 
        }
    )
    
    if not os.path.exists(INPUT_FILE):
        print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {INPUT_FILE}")
        return

    # è¨ˆç®—ç¸½è¡Œæ•¸ä»¥ä¾¿é¡¯ç¤ºé€²åº¦æ¢
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)

    print(f"ğŸš€ ä½¿ç”¨æ¨¡å‹ [{MODEL_NAME}] é€é OpenRouter é–‹å§‹è™•ç†...")

    with open(INPUT_FILE, 'r', encoding='utf-8') as f_in, \
         open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        
        for line in tqdm(f_in, total=total_lines):
            try:
                line = line.strip()
                if not line: continue
                
                data = json.loads(line)
                
                # è³‡æ–™çµæ§‹è§£æ (åŠ ä¸Šå®¹éŒ¯)
                content = None
                q_id = None
                
                # å˜—è©¦å¾æ¨™æº–çµæ§‹è®€å–
                if "query" in data and isinstance(data["query"], dict):
                    content = data["query"].get("content")
                    q_id = data["query"].get("query_id")
                # å˜—è©¦å¾æ‰å¹³çµæ§‹è®€å– (å¦‚æœæœ‰çš„è©±)
                elif "content" in data:
                    content = data["content"]
                    q_id = data.get("query_id")

                if content:
                    # å‘¼å« API
                    keywords = extract_keywords_with_openrouter(client, content)
                    
                    # å»ºç«‹æ–°è³‡æ–™
                    new_record = {
                        "query_id": q_id,
                        "content": content,
                        "keywords": keywords
                    }
                    
                    f_out.write(json.dumps(new_record, ensure_ascii=False) + "\n")
                    
            except json.JSONDecodeError:
                continue
            except Exception as e:
                print(f"è™•ç†å–®è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
                continue

    print(f"\nâœ… è™•ç†å®Œæˆï¼æª”æ¡ˆå·²å„²å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    process_dataset()
