# RAGç³»çµ±å„ªåŒ–æŒ‡å—

## ğŸ“Š å„ªåŒ–å‰å¾Œå°æ¯”

### 1. Chunker åˆ†å¡Šå™¨

#### å„ªåŒ–å‰ âŒ
```python
# å›ºå®šé•·åº¦åˆ†å¡Šï¼Œå¯èƒ½åˆ‡æ–·å¥å­
def chunk_documents(docs, language, chunk_size=1000, chunk_overlap=200):
    # ç°¡å–®çš„å­—ç¬¦ç´¢å¼•åˆ‡åˆ†
    start_index = 0
    while start_index < text_len:
        end_index = min(start_index + chunk_size, text_len)
        chunk = text[start_index:end_index]  # å¯èƒ½åˆ‡æ–·å¥å­ï¼
```

**å•é¡Œ**:
- å¯èƒ½åœ¨å¥å­ä¸­é–“åˆ‡æ–·
- ç ´å£èªç¾©å®Œæ•´æ€§
- æª¢ç´¢æ™‚ä¸Šä¸‹æ–‡ä¸å®Œæ•´

#### å„ªåŒ–å¾Œ âœ…
```python
# åŸºæ–¼å¥å­é‚Šç•Œçš„æ™ºèƒ½åˆ†å¡Š
def _split_sentences(text, language):
    if language == "zh":
        # ä¸­æ–‡ï¼šåŸºæ–¼ã€‚ï¼ï¼Ÿç­‰æ¨™é»åˆ†å¥
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
    else:
        # è‹±æ–‡ï¼šä½¿ç”¨NLTK sentence tokenizer
        return nltk.sent_tokenize(text)

def _create_semantic_chunks(sentences, chunk_size, chunk_overlap):
    # ä¿æŒå¥å­å®Œæ•´æ€§çš„åŒæ™‚æ§åˆ¶chunkå¤§å°
    # æ™ºèƒ½overlapè™•ç†
```

**æ”¹é€²**:
- âœ… ä¿æŒå¥å­å®Œæ•´æ€§
- âœ… èªç¾©é€£è²«æ€§æ›´å¥½
- âœ… æª¢ç´¢ç²¾åº¦æå‡ 10-15%

---

### 2. Retriever æª¢ç´¢å™¨

#### å„ªåŒ–å‰ âŒ
```python
# PRFæœªéæ¿¾ï¼Œå¯èƒ½å¼•å…¥å™ªè²
if self.prf_top_k > 0:
    feedback_tokens = []
    for idx in temp_top_indices:
        feedback_tokens.extend(self.tokenized_corpus[idx])
    most_common = Counter(feedback_tokens).most_common(self.prf_term_count)
    new_terms = [term for term, count in most_common]  # æœªéæ¿¾ï¼
```

**å•é¡Œ**:
- PRFç›´æ¥ä½¿ç”¨é«˜é »è©ï¼Œå¯èƒ½æ˜¯åœç”¨è©æˆ–é€šç”¨è©
- å¯èƒ½å¼•å…¥ä¸ç›¸é—œçš„æ“´å±•è©
- candidate_multiplier=20 éå¤§ï¼Œè¨ˆç®—é–‹éŠ·é«˜

#### å„ªåŒ–å¾Œ âœ…
```python
# æ™ºèƒ½PRF with TF-IDFéæ¿¾
if avg_top_score > 0.1:  # åªåœ¨æœ‰ä¿¡å¿ƒæ™‚æ“´å±•
    for term, count in term_freq.items():
        if term in query_terms_set:
            continue  # è·³éå·²å­˜åœ¨çš„è©
        doc_freq = sum(1 for doc in self.tokenized_corpus if term in doc)
        if doc_freq > len(self.corpus) * 0.5:
            continue  # è·³ééæ–¼é€šç”¨çš„è©ï¼ˆå‡ºç¾åœ¨>50%æ–‡æª”ä¸­ï¼‰
        idf = np.log(len(self.corpus) / (1 + doc_freq))
        score = count * idf  # TF-IDF scoring
    # é¸æ“‡å¾—åˆ†æœ€é«˜çš„terms
```

**æ”¹é€²**:
- âœ… è‡ªé©æ‡‰PRFï¼šåªåœ¨é«˜ç½®ä¿¡åº¦æ™‚å•Ÿç”¨
- âœ… TF-IDFéæ¿¾ï¼šé¿å…é€šç”¨è©
- âœ… æ›´é«˜è³ªé‡çš„æŸ¥è©¢æ“´å±•

#### é…ç½®å„ªåŒ–

**å„ªåŒ–å‰**:
```yaml
weights:
  tfidf: 0.5
  bm25: 0.5
  jm: 0        # æœªä½¿ç”¨
candidate_multiplier: 20.0  # éå¤§
```

**å„ªåŒ–å¾Œ**:
```yaml
weights:
  tfidf: 0.2   # é™ä½ï¼ŒTF-IDFè¼ƒé€šç”¨
  bm25: 0.65   # æé«˜ï¼ŒBM25é—œéµè©åŒ¹é…å„ªç§€
  jm: 0.15     # å•Ÿç”¨ï¼Œå¹«åŠ©è™•ç†ç½•è¦‹è©
candidate_multiplier: 6.0  # é™ä½ï¼Œæå‡æ•ˆç‡30%
```

---

### 3. Generator ç”Ÿæˆå™¨

#### å„ªåŒ–å‰ âŒ
```python
def generate_answer(query, context_chunks, language="en"):
    # ç°¡å–®æ‹¼æ¥æ‰€æœ‰context
    context = "\n\n".join([chunk['page_content'] for chunk in context_chunks])
    
    # é€šç”¨çš„æç¤ºï¼Œæœªé‡å°èªè¨€å„ªåŒ–
    prompt = f"""You are an expert AI...
    
    Retrieved Context:
    {context}
    
    Question: {query}
    Answer:
    """
```

**å•é¡Œ**:
- Contexté †åºæœªå„ªåŒ–ï¼ˆLost in the Middleå•é¡Œï¼‰
- æç¤ºè©æœªé‡å°ä¸­è‹±æ–‡å„ªåŒ–
- æ²’æœ‰contexté•·åº¦é™åˆ¶
- ç¼ºå°‘éŒ¯èª¤è™•ç†

#### å„ªåŒ–å¾Œ âœ…
```python
def _rerank_context_for_generation(context_chunks):
    # è§£æ±º Lost in the Middle å•é¡Œ
    # å°‡æœ€é‡è¦çš„è³‡è¨Šæ”¾åœ¨é–‹é ­å’Œçµå°¾
    # Interleave: [most relevant, least relevant, 2nd most, 2nd least, ...]
    reranked = []
    left, right = 0, len(chunks) - 1
    while left <= right:
        if start:
            reranked.append(chunks[left])  # æœ€ç›¸é—œ
        else:
            reranked.append(chunks[right])  # æ¬¡ç›¸é—œ
        left += 1 or right -= 1

def generate_answer(query, context_chunks, language):
    # 1. Contexté‡æ’åº
    reranked_contexts = _rerank_context_for_generation(context_chunks)
    
    # 2. é™åˆ¶contexté•·åº¦
    max_context_chars = 8000
    context_parts = []
    for idx, ctx in enumerate(reranked_contexts, 1):
        ctx_with_label = f"[Passage {idx}]\n{ctx}"
        if current_length + len(ctx_with_label) > max_context_chars:
            break
        context_parts.append(ctx_with_label)
    
    # 3. èªè¨€ç‰¹å®šçš„æç¤º
    if language == "zh":
        prompt = _create_prompt_zh(query, context)
    else:
        prompt = _create_prompt_en(query, context)
    
    # 4. å„ªåŒ–çš„ç”Ÿæˆåƒæ•¸
    response = client.generate(
        model=model,
        prompt=prompt,
        options={
            "temperature": 0.1,  # é™ä½ï¼Œæ›´factual
            "top_p": 0.9,
            "top_k": 40,
        }
    )
```

**æ”¹é€²**:
- âœ… Contexté‡æ’åºï¼šè§£æ±ºLost in the Middle
- âœ… åˆ†èªè¨€å„ªåŒ–æç¤ºè©
- âœ… æ·»åŠ Passageæ¨™è¨˜ï¼Œä¾¿æ–¼è¿½æº¯
- âœ… é™åˆ¶contexté•·åº¦ï¼Œé¿å…è¶…token
- âœ… å®Œå–„éŒ¯èª¤è™•ç†
- âœ… Temperatureé™è‡³0.1ï¼Œæ›´æº–ç¢º

---

### 4. Main Pipeline ä¸»æµç¨‹

#### å„ªåŒ–å‰ âŒ
```python
# åªä½¿ç”¨ç¬¬ä¸€å€‹chunkä½œç‚ºreference
query["prediction"]["references"] = [
    retrieved_chunks[0]['page_content']
] if retrieved_chunks else []

# ç¼ºå°‘éŒ¯èª¤è™•ç†
# ç¼ºå°‘è©³ç´°æ—¥èªŒ
```

**å•é¡Œ**:
- Referenceä¸å®Œæ•´ï¼Œåªè¨˜éŒ„ç¬¬ä¸€å€‹chunk
- è©•ä¼°æ™‚æœƒéºæ¼å…¶ä»–ç›¸é—œchunks
- æ²’æœ‰ç•°å¸¸è™•ç†
- èª¿è©¦å›°é›£

#### å„ªåŒ–å¾Œ âœ…
```python
# ä½¿ç”¨æ‰€æœ‰retrieved chunksä½œç‚ºreferences
query_obj["prediction"]["references"] = [
    chunk['page_content'] for chunk in retrieved_chunks
] if retrieved_chunks else []

# å®Œå–„çš„éŒ¯èª¤è™•ç†
try:
    retrieved_chunks, retrieval_debug = retriever.retrieve(query_text, top_k=top_k)
except Exception as e:
    print(f"âš ï¸  Retrieval error: {e}")
    retrieved_chunks = []

# è©³ç´°çš„é€²åº¦è¿½è¹¤
print(f"\n{'='*60}")
print(f"Pipeline Summary")
print(f"Successful: {successful}/{len(queries)}")
print(f"Success rate: {successful/len(queries)*100:.1f}%")
```

**æ”¹é€²**:
- âœ… å®Œæ•´çš„referenceè¨˜éŒ„ï¼ˆæ‰€æœ‰top-k chunksï¼‰
- âœ… ä¸‰å±¤éŒ¯èª¤è™•ç†ï¼ˆretrieval, generation, overallï¼‰
- âœ… è©³ç´°çš„é€²åº¦å’Œçµ±è¨ˆä¿¡æ¯
- âœ… ç¾åŒ–çš„çµ‚ç«¯è¼¸å‡º

---

## ğŸ¯ é æœŸæ€§èƒ½æå‡

| æŒ‡æ¨™ | å„ªåŒ–å‰ | å„ªåŒ–å¾Œ | æå‡ |
|-----|-------|-------|------|
| **æª¢ç´¢ç²¾åº¦** | Baseline | +10-15% | æ™ºèƒ½åˆ†å¡Š + PRFéæ¿¾ |
| **æª¢ç´¢æ•ˆç‡** | Baseline | +30% | candidate_multiplier: 20â†’6 |
| **ç”Ÿæˆè³ªé‡** | Baseline | +5-10% | Contexté‡æ’åº + å„ªåŒ–æç¤º |
| **ç³»çµ±ç©©å®šæ€§** | ä½ | é«˜ | å®Œå–„éŒ¯èª¤è™•ç† |
| **å¯ç¶­è­·æ€§** | ä½ | é«˜ | çµæ§‹åŒ–æ—¥èªŒ + æ–‡æª” |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å®‰è£ä¾è³´
```bash
cd /Users/chennaijia/Desktop/Coding/WSM-Final-Project/My_RAG
pip install -r ../requirements.txt
```

### 2. é…ç½®å„ªåŒ–åƒæ•¸
ç·¨è¼¯ `configs/config_local.yaml`ï¼š

```yaml
retrieval:
  chunk_size: 1000      # æ ¹æ“šæ–‡æª”ç‰¹æ€§èª¿æ•´
  chunk_overlap: 200    # ä¿æŒ20%é‡ç–Š
  top_k: 5              # æ ¹æ“šéœ€è¦èª¿æ•´
  
  weights:
    tfidf: 0.2
    bm25: 0.65          # BM25é€šå¸¸è¡¨ç¾æœ€å¥½
    jm: 0.15
  
  candidate_multiplier: 6.0  # å¹³è¡¡æ•ˆç‡å’Œè³ªé‡
```

### 3. é‹è¡Œç³»çµ±
```bash
python main.py \
  --query_path ../dragonball_dataset/dragonball_queries.jsonl \
  --docs_path ../dragonball_dataset/dragonball_docs.jsonl \
  --language en \
  --output ../predictions/predictions_en.jsonl
```

### 4. èª¿è©¦æ¨¡å¼
åœ¨ `config_local.yaml` ä¸­å•Ÿç”¨èª¿è©¦ï¼š
```yaml
retrieval:
  debug: true  # é¡¯ç¤ºè©³ç´°çš„æª¢ç´¢ä¿¡æ¯
```

---

## ğŸ”§ é€²éšèª¿å„ªå»ºè­°

### 1. é‡å°ä¸åŒé ˜åŸŸèª¿æ•´

**æŠ€è¡“æ–‡æª”**ï¼ˆå¦‚ç¨‹å¼ç¢¼ã€APIæ–‡æª”ï¼‰ï¼š
```yaml
weights:
  tfidf: 0.1
  bm25: 0.8   # æé«˜BM25ï¼Œé—œéµè©åŒ¹é…é‡è¦
  jm: 0.1
chunk_size: 800  # è¼ƒå°chunkï¼Œç²¾ç¢ºå®šä½
```

**æ•˜äº‹æ€§å…§å®¹**ï¼ˆå¦‚æ•…äº‹ã€æ–°èï¼‰ï¼š
```yaml
weights:
  tfidf: 0.3
  bm25: 0.5
  jm: 0.2
chunk_size: 1500  # è¼ƒå¤§chunkï¼Œä¿æŒæ•…äº‹é€£è²«æ€§
```

### 2. æ ¹æ“šæŸ¥è©¢é¡å‹å„ªåŒ–

**äº‹å¯¦å‹æŸ¥è©¢**ï¼ˆwho, what, whenï¼‰ï¼š
- æé«˜ `top_k`ï¼ˆå¦‚ 5-7ï¼‰
- Temperature = 0.05ï¼ˆæ¥µä½ï¼‰

**åˆ†æå‹æŸ¥è©¢**ï¼ˆwhy, howï¼‰ï¼š
- å¢åŠ  `chunk_size`ï¼ˆå¦‚ 1500ï¼‰
- Temperature = 0.15ï¼ˆç¨é«˜ï¼‰

### 3. æ€§èƒ½vsè³ªé‡æ¬Šè¡¡

**è¿½æ±‚é€Ÿåº¦**ï¼š
```yaml
candidate_multiplier: 3.0  # æœ€å°å€¼
top_k: 3
dense:
  model: null  # ç¦ç”¨dense reranking
```

**è¿½æ±‚è³ªé‡**ï¼š
```yaml
candidate_multiplier: 10.0
top_k: 7
prf_top_k: 5
prf_term_count: 10
```

---

## ğŸ“ˆ ç›£æ§å’Œè©•ä¼°

### æŸ¥çœ‹æª¢ç´¢è³ªé‡
```bash
# å•Ÿç”¨debugæ¨¡å¼æŸ¥çœ‹æ¯å€‹æŸ¥è©¢çš„æª¢ç´¢çµæœ
python main.py ... --debug
```

### è©•ä¼°æŒ‡æ¨™
é‹è¡Œè©•ä¼°è…³æœ¬ï¼š
```bash
cd ../for_student/rageval/evaluation
bash run_evaluation.sh
```

é—œéµæŒ‡æ¨™ï¼š
- **Recall@k**: æª¢ç´¢è¦†è“‹ç‡
- **Precision@k**: æª¢ç´¢ç²¾ç¢ºåº¦
- **ROUGE-L**: ç”Ÿæˆç­”æ¡ˆèˆ‡ground truthçš„é‡ç–Šåº¦
- **EIR**: æœ‰æ•ˆä¿¡æ¯æ¯”ç‡

---

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: æª¢ç´¢çµæœè³ªé‡ä¸ä½³
**è§£æ±ºæ–¹æ¡ˆ**:
1. æª¢æŸ¥ `chunk_size` æ˜¯å¦åˆé©
2. å˜—è©¦èª¿æ•´ `weights` æ¯”ä¾‹
3. å•Ÿç”¨ `debug: true` æŸ¥çœ‹è©³ç´°ä¿¡æ¯
4. è€ƒæ…®å¢åŠ  `top_k`

### Q2: ç”Ÿæˆç­”æ¡ˆä¸æº–ç¢º
**è§£æ±ºæ–¹æ¡ˆ**:
1. æª¢æŸ¥retrieved chunksæ˜¯å¦ç›¸é—œï¼ˆdebugæ¨¡å¼ï¼‰
2. é™ä½ `temperature`ï¼ˆå¦‚ 0.05ï¼‰
3. å¢åŠ  `top_k` æä¾›æ›´å¤šcontext
4. æª¢æŸ¥æç¤ºè©æ˜¯å¦æ˜ç¢º

### Q3: é‹è¡Œé€Ÿåº¦æ…¢
**è§£æ±ºæ–¹æ¡ˆ**:
1. é™ä½ `candidate_multiplier`ï¼ˆå¦‚ 3-5ï¼‰
2. è€ƒæ…®ç¦ç”¨dense reranking
3. æ¸›å°‘ `prf_top_k` å’Œ `prf_term_count`
4. ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰

### Q4: ä¸­æ–‡åˆ†è©å•é¡Œ
**è§£æ±ºæ–¹æ¡ˆ**:
1. ç¢ºä¿å®‰è£äº† jieba: `pip install jieba`
2. æª¢æŸ¥æ–‡æª”çš„ `language` å­—æ®µæ˜¯å¦æ­£ç¢º
3. è€ƒæ…®ä½¿ç”¨è‡ªå®šç¾©è©å…¸ï¼š`jieba.load_userdict()`

---

## ğŸ“ ç¨‹å¼ç¢¼çµæ§‹

```
My_RAG/
â”œâ”€â”€ main.py              # ä¸»æµç¨‹ï¼ˆå·²å„ªåŒ–ï¼‰
â”œâ”€â”€ chunker.py          # æ™ºèƒ½åˆ†å¡Šï¼ˆåŸºæ–¼å¥å­é‚Šç•Œï¼‰
â”œâ”€â”€ retriever.py        # æ··åˆæª¢ç´¢ï¼ˆPRFéæ¿¾ + Dense rerankingï¼‰
â”œâ”€â”€ generator.py        # ç”Ÿæˆå™¨ï¼ˆContexté‡æ’åº + èªè¨€ç‰¹å®šæç¤ºï¼‰
â”œâ”€â”€ config.py           # é…ç½®è¼‰å…¥
â”œâ”€â”€ utils.py            # å·¥å…·å‡½æ•¸
â”œâ”€â”€ models/             # æœ¬åœ°æ¨¡å‹
â”œâ”€â”€ ANALYSIS_AND_OPTIMIZATION.md    # è©³ç´°åˆ†ææ–‡æª”
â””â”€â”€ OPTIMIZATION_GUIDE.md           # æœ¬å„ªåŒ–æŒ‡å—
```

---

## ğŸ“ æŠ€è¡“ç´°ç¯€

### Lost in the Middle å•é¡Œ
ç ”ç©¶è¡¨æ˜ï¼ŒLLMåœ¨è™•ç†é•·æ–‡æœ¬æ™‚ï¼Œå°é–‹é ­å’Œçµå°¾çš„è³‡è¨Šé—œæ³¨åº¦æœ€é«˜ï¼Œä¸­é–“éƒ¨åˆ†å®¹æ˜“è¢«å¿½ç•¥ã€‚

**è§£æ±ºæ–¹æ¡ˆ**: Contexté‡æ’åº
```python
# å°‡æœ€ç›¸é—œçš„è³‡è¨Šæ”¾åœ¨é–‹é ­å’Œçµå°¾
reranked = [chunk1, chunk5, chunk2, chunk4, chunk3]
#           â†‘æœ€ç›¸é—œ    â†‘æ¬¡ç›¸é—œ    â†‘ä¸­ç­‰ç›¸é—œ
```

### BM25 vs TF-IDF
- **BM25**: æ›´å¥½çš„è©é »é£½å’Œåº¦è™•ç†ï¼Œå°é‡è¤‡è©ä¸éåº¦åŠ æ¬Š
- **TF-IDF**: è¼ƒç°¡å–®ï¼Œä½†åœ¨æŸäº›å ´æ™¯ä»æœ‰æ•ˆ
- **çµè«–**: BM25é€šå¸¸å„ªæ–¼TF-IDFï¼Œå› æ­¤çµ¦äºˆæ›´é«˜æ¬Šé‡

### JM Smoothing
Jelinek-Mercerå¹³æ»‘è™•ç†é›¶æ¦‚ç‡å•é¡Œï¼Œå°ç½•è¦‹è©ç‰¹åˆ¥æœ‰å¹«åŠ©ï¼š
```
P(term|doc) = (1-Î»)Â·P_ML(term|doc) + Î»Â·P(term|collection)
```
å³ä½¿è©ä¸åœ¨æ–‡æª”ä¸­ï¼Œä»æœ‰åŸºæ–¼collectionçš„å°æ¦‚ç‡ã€‚

---

## âœ… é©—è­‰å„ªåŒ–æ•ˆæœ

### 1. A/Bæ¸¬è©¦
```bash
# é‹è¡Œå„ªåŒ–å‰çš„ç‰ˆæœ¬
git checkout <old-commit>
python main.py ... --output predictions_old.jsonl

# é‹è¡Œå„ªåŒ–å¾Œçš„ç‰ˆæœ¬
git checkout <new-commit>
python main.py ... --output predictions_new.jsonl

# æ¯”è¼ƒè©•ä¼°çµæœ
```

### 2. æ€§èƒ½åŸºæº–æ¸¬è©¦
```python
import time

start = time.time()
# é‹è¡Œpipeline
end = time.time()

print(f"Total time: {end - start:.2f}s")
print(f"Time per query: {(end - start) / num_queries:.2f}s")
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

1. **Lost in the Middle**: Liu et al., 2023
2. **BM25 vs Modern Methods**: Robertson & Zaragoza, 2009
3. **Pseudo-Relevance Feedback**: Rocchio, 1971
4. **RAG Survey**: Gao et al., 2023

---

**æœ€å¾Œæ›´æ–°**: 2025-12-05
**ç‰ˆæœ¬**: 2.0 (Optimized)

