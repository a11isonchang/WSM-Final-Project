"""
æ„å»ºçŸ¥è¯†å›¾è°± (Knowledge Graph) ä» dragonball_docs.jsonl
ä½¿ç”¨ LLM ä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºæˆçŸ¥è¯†å›¾è°±æ ¼å¼

ç‰ˆæœ¬èªªæ˜ï¼š
- åŸæœ¬ä½¿ç”¨ Ollamaï¼Œå·²æ”¹ç‚ºä½¿ç”¨ OpenAI APIï¼ˆopenai.ChatCompletionï¼‰
- æ¨¡å‹å»ºè­°ï¼šgpt-4o-miniï¼ˆå¯åœ¨ config.yaml æˆ–ç’°å¢ƒè®Šæ•¸è¨­å®šï¼‰
"""

import json
import os
from typing import List, Dict, Any, Set
from pathlib import Path

from tqdm import tqdm
from openai import OpenAI

from config import load_config
from utils import load_jsonl, save_jsonl

# =========================
# OpenAI è¨­å®šèˆ‡ Client
# =========================

_openai_client = None
_openai_model = None


def init_openai_from_config():
    """
    åˆå§‹åŒ– OpenAI Client èˆ‡æ¨¡å‹åç¨±ï¼š
    - model ä¾†æºï¼šconfig.yaml çš„ openai.model æˆ–ç’°å¢ƒè®Šæ•¸ OPENAI_MODELï¼Œé è¨­ gpt-4o-mini
    - api key ä¾†æºï¼šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY æˆ– config.yaml çš„ openai.api_key_env
    """
    global _openai_client, _openai_model

    if _openai_client is not None and _openai_model is not None:
        return

    config = load_config()
    openai_cfg = config.get("openai", {})

    model = openai_cfg.get("model", os.environ.get("OPENAI_MODEL", "gpt-4o-mini"))
    api_key_env = openai_cfg.get("api_key_env", "OPENAI_API_KEY")
    api_key = os.environ.get(api_key_env)

    if not api_key:
        raise RuntimeError(
            f"OpenAI API key not found. Please set environment variable '{api_key_env}'."
        )

    _openai_client = OpenAI(api_key=api_key)
    _openai_model = model


def get_openai_client():
    """å–å¾—å…¨åŸŸå–®ä¾‹ OpenAI Client"""
    if _openai_client is None or _openai_model is None:
        init_openai_from_config()
    return _openai_client


def get_openai_model() -> str:
    """å–å¾—ç›®å‰ä½¿ç”¨çš„ OpenAI æ¨¡å‹åç¨±"""
    if _openai_client is None or _openai_model is None:
        init_openai_from_config()
    return _openai_model


# =========================
# JSON ä¿®å¾©èˆ‡æå–è¼”åŠ©å‡½æ•¸
# =========================

def _extract_json_from_text(text: str) -> str:
    """
    å¾æ–‡æœ¬ä¸­æå– JSONï¼Œä½¿ç”¨å¤šç¨®ç­–ç•¥
    """
    # ç­–ç•¥1: æå– ```json ... ``` ä¸­çš„å…§å®¹
    if "```json" in text:
        parts = text.split("```json", 1)
        if len(parts) > 1:
            json_part = parts[1].split("```", 1)[0].strip()
            return json_part
    
    # ç­–ç•¥2: æå– ``` ... ``` ä¸­çš„å…§å®¹
    if "```" in text:
        parts = text.split("```", 1)
        if len(parts) > 1:
            json_part = parts[1].split("```", 1)[0].strip()
            # å¦‚æœä¸æ˜¯ JSONï¼Œç¹¼çºŒå˜—è©¦å…¶ä»–ç­–ç•¥
            if json_part.startswith("{"):
                return json_part
    
    # ç­–ç•¥3: æŸ¥æ‰¾ç¬¬ä¸€å€‹ { åˆ°æœ€å¾Œä¸€å€‹ } ä¹‹é–“çš„å…§å®¹
    first_brace = text.find("{")
    last_brace = text.rfind("}")
    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
        return text[first_brace:last_brace + 1]
    
    # ç­–ç•¥4: è¿”å›åŸå§‹æ–‡æœ¬
    return text.strip()


from concurrent.futures import ThreadPoolExecutor, as_completed

def _process_single_doc(doc: Dict[str, Any], default_language: str) -> tuple[int | None, Dict[str, Any] | None, str | None]:
    """
    ä¾› ThreadPoolExecutor ä½¿ç”¨çš„ workerã€‚
    ä¸æ”¹ä»»ä½•æŠ½å–é‚è¼¯ï¼Œåªæ˜¯æŠŠåŸæœ¬ for è¿´åœˆè£¡çš„ per-doc è™•ç†åŒ…æˆä¸€å€‹å‡½å¼ã€‚

    å›å‚³:
        (doc_id, kg_result or None, error_message or None)
    """
    doc_id = doc.get("doc_id")
    if doc_id is None:
        return None, None, "doc_id is None"

    domain = doc.get("domain", "Finance")  # default: Finance
    content = doc.get("content", "")
    doc_language = doc.get("language", default_language)

    if not content:
        return doc_id, None, "empty content"

    # æ ¹æ“š domain å–åç¨±æ¬„ä½
    if domain == "Finance":
        domain_name = doc.get("company_name", "")
    elif domain == "Law":
        domain_name = doc.get("court_name", "")
    elif domain == "Medical":
        domain_name = doc.get("hospital_patient_name", "")
    else:
        domain_name = doc.get(
            "company_name",
            doc.get("court_name", doc.get("hospital_patient_name", "")),
        )

    try:
        kg_result = extract_entities_and_relations(
            text=content,
            domain=domain,
            domain_name=domain_name,
            doc_id=doc_id,
            language=doc_language,
        )
        return doc_id, kg_result, None
    except Exception as e:
        return doc_id, None, str(e)


def _fix_json_string(json_str: str) -> str:
    """
    å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„ JSON æ ¼å¼å•é¡Œï¼ŒåŒ…æ‹¬æœªçµ‚æ­¢çš„å­—ç¬¦ä¸²
    """
    if not json_str:
        return json_str
    
    # ä¿®å¾©æœªçµ‚æ­¢çš„å­—ç¬¦ä¸²ï¼šå¾å¾Œå¾€å‰æŸ¥æ‰¾ï¼Œæ‰¾åˆ°æœªé–‰åˆçš„å­—ç¬¦ä¸²ä¸¦é—œé–‰å®ƒ
    result = []
    i = 0
    in_string = False
    escape_next = False
    
    while i < len(json_str):
        char = json_str[i]
        
        if escape_next:
            result.append(char)
            escape_next = False
            i += 1
            continue
        
        if char == '\\':
            result.append(char)
            escape_next = True
            i += 1
            continue
        
        if char == '"':
            result.append(char)
            in_string = not in_string
            i += 1
            continue
        
        result.append(char)
        i += 1
    
    # å¦‚æœå­—ç¬¦ä¸²æœªçµ‚æ­¢ï¼Œé—œé–‰å®ƒ
    if in_string:
        result.append('"')
    
    fixed = ''.join(result)
    
    # ç§»é™¤å¯èƒ½çš„å°¾éš¨é€—è™Ÿï¼ˆåœ¨æœ€å¾Œä¸€å€‹å…ƒç´ å¾Œï¼‰
    lines = fixed.split('\n')
    fixed_lines = []
    for i, line in enumerate(lines):
        stripped = line.rstrip()
        # ç§»é™¤è¡Œå°¾çš„é€—è™Ÿï¼ˆå¦‚æœä¸‹ä¸€è¡Œæ˜¯ } æˆ– ]ï¼‰
        if i < len(lines) - 1:
            next_stripped = lines[i + 1].strip()
            if stripped.endswith(',') and (next_stripped.startswith('}') or next_stripped.startswith(']')):
                # æª¢æŸ¥æ˜¯å¦åœ¨å­—ç¬¦ä¸²å…§ï¼ˆç°¡å–®æª¢æŸ¥ï¼‰
                quote_count = stripped.count('"') - stripped.count('\\"')
                if quote_count % 2 == 0:  # ä¸åœ¨å­—ç¬¦ä¸²å…§
                    stripped = stripped[:-1]
        fixed_lines.append(stripped)
    
    return '\n'.join(fixed_lines)


def _try_parse_json(json_str: str, max_attempts: int = 3) -> Dict[str, Any]:
    """
    å˜—è©¦è§£æ JSONï¼Œå¦‚æœå¤±æ•—å‰‡å˜—è©¦ä¿®å¾©
    """
    # ç¬¬ä¸€æ¬¡å˜—è©¦ï¼šç›´æ¥è§£æ
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        pass
    
    # ç¬¬äºŒæ¬¡å˜—è©¦ï¼šä¿®å¾©å¾Œè§£æ
    try:
        fixed = _fix_json_string(json_str)
        return json.loads(fixed)
    except json.JSONDecodeError:
        pass
    
    # ç¬¬ä¸‰æ¬¡å˜—è©¦ï¼šæŸ¥æ‰¾ä¸¦æå–å®Œæ•´çš„ JSON å°è±¡ï¼Œè™•ç†æˆªæ–·æƒ…æ³
    try:
        # å˜—è©¦æ‰¾åˆ°å®Œæ•´çš„ entities å’Œ relations çµæ§‹
        if '"entities"' in json_str and '"relations"' in json_str:
            # æå– entities éƒ¨åˆ†
            entities_start = json_str.find('"entities"')
            relations_start = json_str.find('"relations"')
            
            if entities_start != -1 and relations_start != -1:
                # æ§‹å»ºä¸€å€‹æœ€å°å¯ç”¨çš„ JSON
                entities_part = json_str[entities_start:relations_start]
                relations_part = json_str[relations_start:]
                
                # å˜—è©¦æå–å®Œæ•´çš„æ•¸çµ„
                entities_bracket = entities_part.find('[')
                relations_bracket = relations_part.find('[')
                
                if entities_bracket != -1 and relations_bracket != -1:
                    entities_match = entities_part[entities_bracket:]
                    relations_match = relations_part[relations_bracket:]
                    
                    # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„ ]ï¼Œå¦‚æœæ²’æœ‰å‰‡å˜—è©¦é—œé–‰å®ƒ
                    entities_end = entities_match.rfind(']')
                    relations_end = relations_match.rfind(']')
                    
                    # å¦‚æœ entities æ•¸çµ„æœªé–‰åˆï¼Œå˜—è©¦é—œé–‰å®ƒ
                    if entities_end == -1:
                        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„å°è±¡
                        last_comma = entities_match.rfind(',')
                        if last_comma != -1:
                            # ç§»é™¤æœ€å¾Œçš„é€—è™Ÿä¸¦é—œé–‰æ•¸çµ„
                            entities_str = entities_match[:last_comma] + ']'
                        else:
                            # å¦‚æœæ²’æœ‰é€—è™Ÿï¼Œç›´æ¥é—œé–‰
                            entities_str = entities_match.rstrip().rstrip(',') + ']'
                    else:
                        entities_str = entities_match[:entities_end + 1]
                    
                    # å¦‚æœ relations æ•¸çµ„æœªé–‰åˆï¼Œå˜—è©¦é—œé–‰å®ƒ
                    if relations_end == -1:
                        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„å°è±¡
                        last_comma = relations_match.rfind(',')
                        if last_comma != -1:
                            # ç§»é™¤æœ€å¾Œçš„é€—è™Ÿä¸¦é—œé–‰æ•¸çµ„
                            relations_str = relations_match[:last_comma] + ']'
                        else:
                            # å¦‚æœæ²’æœ‰é€—è™Ÿï¼Œç›´æ¥é—œé–‰
                            relations_str = relations_match.rstrip().rstrip(',') + ']'
                    else:
                        relations_str = relations_match[:relations_end + 1]
                    
                    # æ§‹å»ºå®Œæ•´çš„ JSON
                    fixed_json = f'{{"entities": {entities_str}, "relations": {relations_str}}}'
                    return json.loads(fixed_json)
        elif '"entities"' in json_str:
            # åªæœ‰ entitiesï¼Œæ²’æœ‰ relationsï¼ˆå¯èƒ½è¢«æˆªæ–·ï¼‰
            entities_start = json_str.find('"entities"')
            if entities_start != -1:
                entities_part = json_str[entities_start:]
                entities_bracket = entities_part.find('[')
                if entities_bracket != -1:
                    entities_match = entities_part[entities_bracket:]
                    entities_end = entities_match.rfind(']')
                    if entities_end != -1:
                        entities_str = entities_match[:entities_end + 1]
                        fixed_json = f'{{"entities": {entities_str}, "relations": []}}'
                        return json.loads(fixed_json)
    except (json.JSONDecodeError, ValueError):
        pass
    
    # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œæ‹‹å‡ºç•°å¸¸
    raise json.JSONDecodeError("Failed to parse JSON after multiple attempts", json_str, 0)


# =========================
# LLM æŠ½å–å¯¦é«”èˆ‡é—œä¿‚
# =========================
def extract_entities_and_relations(
    text: str,
    domain: str,
    domain_name: str,
    doc_id: int,
    language: str = "zh",
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼ˆOpenAI v1 SDK ç‰ˆæœ¬ï¼‰

    Args:
        text: æ–‡æ¡£å†…å®¹
        domain: é¢†åŸŸç±»å‹ ("Finance", "Law", "Medical")
        domain_name: é¢†åŸŸç›¸å…³åç§°ï¼ˆå…¬å¸å/æ³•é™¢å/åŒ»é™¢æ‚£è€…åï¼‰
        doc_id: æ–‡æ¡£ID
        language: è¯­è¨€ ("zh" æˆ– "en")
    """
    # ========= 1. æ–‡æœ¬é•·åº¦æ§åˆ¶ =========
    max_chars = 4000  # æ¯”ä½ åŸæœ¬ 8000 å°ä¸€é»ï¼Œæ¯”è¼ƒçœ token
    if len(text) > max_chars:
        prefix_len = 1200
        suffix_len = 800
        middle_start = (len(text) - prefix_len - suffix_len) // 2

        prefix = text[:prefix_len]
        suffix = text[-suffix_len:]
        middle = text[middle_start:middle_start + (max_chars - prefix_len - suffix_len)]
        text = (
            prefix
            + "\n...[ä¸­é—´å†…å®¹çœç•¥]...\n"
            + middle
            + "\n...[ä¸­é—´å†…å®¹çœç•¥]...\n"
            + suffix
        )
        text = text[:max_chars]

    # ========= 2. æ ¹æ“š domain é¸ prompt =========
    if domain == "Finance":
        prompt = _generate_finance_prompt(text, domain_name, language)
    elif domain == "Law":
        prompt = _generate_law_prompt(text, domain_name, language)
    elif domain == "Medical":
        prompt = _generate_medical_prompt(text, domain_name, language)
    else:
        raise ValueError(f"Unsupported domain: {domain}")

    response_text = ""
    try:
        # ========= 3. å»ºç«‹æ–°ç‰ˆ OpenAI Client =========
        client = get_openai_client()
        model = get_openai_model()

        system_msg_zh = "ä½ æ˜¯ä¸€å€‹å°ˆé–€å¾é•·æ–‡æœ¬ä¸­æŠ½å–å¯¦é«”èˆ‡é—œä¿‚ï¼Œä¸¦è¼¸å‡ºä¹¾æ·¨ JSON çµæ§‹çš„çŸ¥è­˜åœ–è­œæ§‹å»ºåŠ©æ‰‹ã€‚"
        system_msg_en = "You are an expert assistant for extracting entities and relations from long documents and outputting clean JSON for a knowledge graph."
        system_content = system_msg_zh if language == "zh" else system_msg_en

        # âœ… æ–°ç‰ˆï¼šclient.chat.completions.create(...)
        # å¢åŠ  max_tokens ä»¥é¿å… JSON è¢«æˆªæ–·
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt},
            ],
            temperature=0.1,
            max_tokens=4000,  # å¾ 1500 å¢åŠ åˆ° 4000ï¼Œé¿å… JSON è¢«æˆªæ–·
        )

        response_text = (resp.choices[0].message.content or "").strip()

        # ========= 4. å¾å›æ‡‰ä¸­æŠ½ JSON =========
        json_str = _extract_json_from_text(response_text)
        
        # ========= 5. è§£æ JSONï¼ˆå¸¶é‡è©¦å’Œä¿®å¾©ï¼‰=========
        result = _try_parse_json(json_str)
        result["doc_id"] = doc_id
        return result

    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse JSON from LLM response for doc_id {doc_id}: {e}")
        if response_text:
            # æ‰“å°æ›´å¤šä¿¡æ¯ä»¥ä¾¿èª¿è©¦
            print(f"Response text length: {len(response_text)}")
            print(f"Response text (first 1000 chars): {response_text[:1000]!r}")
            if len(response_text) > 1000:
                print(f"Response text (last 500 chars): {response_text[-500:]!r}")
        return {"entities": [], "relations": [], "doc_id": doc_id, "error": str(e)}
    except Exception as e:
        print(f"Error extracting entities and relations for doc_id {doc_id}: {e}")
        import traceback
        traceback.print_exc()
        return {"entities": [], "relations": [], "doc_id": doc_id, "error": str(e)}


# =========================
# Prompt ç”Ÿæˆï¼ˆFinance / Law / Medicalï¼‰
# =========================

def _generate_finance_prompt(text: str, company_name: str, language: str) -> str:
    """ç”Ÿæˆ Finance é¢†åŸŸçš„ prompt"""
    if language == "zh":
        return f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å…¬å¸è´¢åŠ¡æŠ¥å‘Šä¸­ç²¾ç¡®æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

å…¬å¸åç§°ï¼š{company_name}

æŠ¥å‘Šå†…å®¹ï¼š
{text}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼ˆæ³¨æ„ï¼šæ¯ä¸ªå®ä½“å¿…é¡»æœ‰ä¸€ä¸ªå”¯ä¸€ä¸”ç²¾ç¡®çš„åç§°ï¼‰ï¼š

1. **å…¬å¸å®ä½“** (Company)ï¼š
   - ä¸»å…¬å¸åç§°ï¼š{company_name}
   - å­å…¬å¸ã€è¢«æ”¶è´­å…¬å¸ã€åˆä½œä¼™ä¼´å…¬å¸
   - ä¾‹å¦‚ï¼šè‰è“æ–‡åŒ–ä¼ åª’æœ‰é™å…¬å¸ã€å˜‰æ‚¦ä¼ åª’æœ‰é™å…¬å¸

2. **äº‹ä»¶å®ä½“** (Event)ï¼š
   - è´¢åŠ¡äº‹ä»¶ï¼šèµ„äº§æ”¶è´­ã€è‚¡æƒæ”¶è´­ã€èèµ„æ´»åŠ¨ã€æŠ•èµ„ã€å€ºåŠ¡é‡ç»„ã€èµ„äº§é‡ç»„ã€è‚¡åˆ©åˆ†å‘
   - æ²»ç†äº‹ä»¶ï¼šé“å¾·ä¸è¯šä¿¡äº‹ä»¶ã€åˆè§„ä¸ç›‘ç®¡æ›´æ–°ã€è‘£äº‹ä¼šå˜æ›´ã€é«˜çº§ç®¡ç†å±‚å˜åŠ¨ã€è‚¡ä¸œå¤§ä¼šå†³è®®ã€å…¬å¸æ²»ç†æ”¿ç­–ä¿®è®¢
   - ç¯å¢ƒè´£ä»»äº‹ä»¶ï¼šç¢³æŠµæ¶ˆé¡¹ç›®ã€èŠ‚èƒ½å‡æ’é¡¹ç›®ã€æ±¡æŸ“é˜²æ²»è®¾æ–½å»ºè®¾ã€ç¯ä¿äº§å“å¼€å‘ã€ç¯å¢ƒç®¡ç†ç³»ç»Ÿå®æ–½ã€ç”Ÿæ€æ¢å¤è®¡åˆ’
   - ç¤¾ä¼šè´£ä»»äº‹ä»¶ï¼šæ…ˆå–„æ´»åŠ¨ã€ç¤¾åŒºæŠ•èµ„ã€å…¬å…±æœåŠ¡é¡¹ç›®ã€å‘˜å·¥å¥åº·ä¸å®‰å…¨è®¡åˆ’ã€å‘˜å·¥èŒä¸šæˆé•¿è®¡åˆ’
   - å¯æŒç»­æ€§ä¸ç¤¾ä¼šè´£ä»»å€¡è®®

3. **æ—¶é—´å®ä½“** (Time)ï¼š
   - å…·ä½“æ—¶é—´ï¼šå¹´ä»½ï¼ˆå¦‚2017å¹´ï¼‰ã€æœˆä»½ï¼ˆå¦‚2017å¹´2æœˆï¼‰ã€æ—¥æœŸ
   - æ—¶é—´èŒƒå›´ï¼š2017å¹´åº¦ã€2020å¹´è‡³2022å¹´

4. **è´¢åŠ¡æŒ‡æ ‡å®ä½“** (FinancialMetric)ï¼š
   - è¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦ã€æ€»èµ„äº§ã€æ€»è´Ÿå€ºã€è‚¡ä¸œæƒç›Šã€ç°é‡‘æµé‡
   - è´Ÿå€ºæ¯”ç‡ã€èµ„äº§è´Ÿå€ºç‡ã€å‡€èµ„äº§æ”¶ç›Šç‡

5. **äººç‰©å®ä½“** (Person)ï¼š
   - é«˜ç®¡èŒä½ï¼šCEOã€CFOã€CTOã€è‘£äº‹é•¿
   - å…·ä½“äººç‰©ï¼ˆå¦‚æœæåŠå§“åï¼‰

6. **é¡¹ç›®å®ä½“** (Project)ï¼š
   - æŠ•èµ„é¡¹ç›®ï¼šDé¡¹ç›®ã€æ ¸ç”µç«™é¡¹ç›®ã€ç»¿è‰²èƒ½æºç§‘æŠ€å›­
   - å»ºè®¾é¡¹ç›®ã€ç ”å‘é¡¹ç›®

7. **åœ°ç‚¹å®ä½“** (Location)ï¼š
   - æ³¨å†Œåœ°ï¼šä¸Šæµ·ã€åŒ—äº¬ã€äº‘å—ã€ç¾å›½åŠ åˆ©ç¦å°¼äºšå·
   - ä¸Šå¸‚åœ°ç‚¹ï¼šä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€ã€çº½çº¦è¯åˆ¸äº¤æ˜“æ‰€

8. **é‡‘é¢å®ä½“** (Amount)ï¼š
   - å…·ä½“é‡‘é¢ï¼š1.2äº¿å…ƒã€5000ä¸‡å…ƒã€10äº¿ç¾å…ƒ

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å…³ç³»ï¼ˆå…³ç³»ç±»å‹ä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼‰ï¼š

- HAPPENED_AT: äº‹ä»¶å‘ç”Ÿåœ¨æŸä¸ªæ—¶é—´ï¼ˆEvent -> Timeï¼‰
- INVOLVES: äº‹ä»¶æ¶‰åŠæŸä¸ªå®ä½“ï¼ˆEvent -> Company/Person/Project/Locationï¼‰
- ACQUIRES: å…¬å¸æ”¶è´­å…¶ä»–å…¬å¸æˆ–èµ„äº§ï¼ˆCompany -> Company/Assetï¼‰
- INVESTS_IN: å…¬å¸æŠ•èµ„é¡¹ç›®æˆ–å…¬å¸ï¼ˆCompany -> Project/Companyï¼‰
- RAISES_FUNDS: å…¬å¸è¿›è¡Œèèµ„æ´»åŠ¨ï¼ˆCompany -> Eventï¼Œé‡‘é¢åœ¨propertiesä¸­ï¼‰
- REORGANIZES: å…¬å¸è¿›è¡Œé‡ç»„ï¼ˆCompany -> Eventï¼Œå¦‚å€ºåŠ¡é‡ç»„ã€èµ„äº§é‡ç»„ï¼‰
- DISTRIBUTES: å…¬å¸åˆ†å‘è‚¡åˆ©ï¼ˆCompany -> Eventï¼Œé‡‘é¢åœ¨propertiesä¸­ï¼‰
- HAS_METRIC: å…¬å¸æ‹¥æœ‰æŸä¸ªè´¢åŠ¡æŒ‡æ ‡å€¼ï¼ˆCompany -> FinancialMetricï¼Œæ•°å€¼åœ¨propertiesä¸­ï¼‰
- OCCURS_AT: äº‹ä»¶å‘ç”Ÿåœ¨æŸä¸ªåœ°ç‚¹ï¼ˆEvent -> Locationï¼‰
- COSTS: äº‹ä»¶/é¡¹ç›®æ¶‰åŠé‡‘é¢ï¼ˆEvent/Project -> Amountï¼‰
- WORKS_AT: äººç‰©åœ¨å…¬å¸ä»»èŒï¼ˆPerson -> Companyï¼‰
- LOCATED_IN: å…¬å¸ä½äºæŸä¸ªåœ°ç‚¹ï¼ˆCompany -> Locationï¼‰
- SUBSIDIARY_OF: å­å…¬å¸å…³ç³»ï¼ˆCompany -> Companyï¼‰
- PART_OF: é¡¹ç›®å±äºæŸä¸ªäº‹ä»¶æˆ–è®¡åˆ’ï¼ˆProject -> Eventï¼‰

é‡è¦æç¤ºï¼š
- æ¯ä¸ªå®ä½“å¿…é¡»æœ‰å”¯ä¸€ä¸”æ¸…æ™°çš„åç§°
- å…³ç³»å¿…é¡»æ˜ç¡®ä¸”æœ‰æ„ä¹‰
- åœ¨propertiesä¸­è®°å½•è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æ—¶é—´ã€é‡‘é¢ã€ç™¾åˆ†æ¯”ç­‰
- åªæå–æ–‡æ¡£ä¸­æ˜ç¡®æåŠçš„å®ä½“å’Œå…³ç³»ï¼Œä¸è¦æ¨æµ‹

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Company",
            "name": "å®ä½“åç§°ï¼ˆç²¾ç¡®ä¸”å”¯ä¸€ï¼‰",
            "properties": {{
                "description": "å®ä½“æè¿°ï¼ˆå¦‚æœ‰ï¼‰",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "å…³ç³»ç±»å‹ï¼ˆä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œå¦‚HAPPENED_ATï¼‰",
            "properties": {{
                "æ—¶é—´": "2017å¹´2æœˆ",
                "é‡‘é¢": "1.2äº¿å…ƒ",
                "ç™¾åˆ†æ¯”": "70%",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ]
}}

åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    else:
        return f"""You are a knowledge graph construction expert. Extract entities and relations from the following company financial report to build a knowledge graph.

Company Name: {company_name}

Report Content:
{text}

Please extract the following types of entities (each entity must have a unique and precise name):

1. **Company** entities:
   - Main company: {company_name}
   - Subsidiaries, acquired companies, partner companies

2. **Event** entities:
   - Financial events: asset acquisition, equity acquisition, financing, investment, debt restructuring, asset restructuring, dividend distribution
   - Governance events: ethics and integrity incidents, compliance and regulatory updates, board changes, senior management changes, shareholder meeting resolutions, governance policy revisions
   - Environmental responsibility events: carbon offset projects, energy-saving projects, pollution prevention facilities, green product development, environmental management systems, ecological restoration plans
   - Social responsibility events: charity activities, community investment, public service projects, employee health and safety programs, employee career development programs
   - Sustainability and social responsibility initiatives

3. **Time** entities:
   - Specific times: years (e.g., 2017), months (e.g., February 2017), dates
   - Time ranges: 2017 fiscal year, 2020-2022

4. **FinancialMetric** entities:
   - Revenue, net profit, total assets, total liabilities, shareholder equity, cash flow
   - Debt ratio, asset-liability ratio, return on equity

5. **Person** entities:
   - Executive positions: CEO, CFO, CTO, Chairman
   - Specific individuals (if names are mentioned)

6. **Project** entities:
   - Investment projects, construction projects, R&D projects

7. **Location** entities:
   - Registered location, listing location (e.g., Shanghai Stock Exchange, New York Stock Exchange)

8. **Amount** entities:
   - Specific amounts: 120 million yuan, 50 million USD, etc.

Please extract the following types of relations (use uppercase English with underscores):

- HAPPENED_AT: events happening at certain times (Event -> Time)
- INVOLVES: events involving certain entities (Event -> Company/Person/Project/Location)
- ACQUIRES: companies acquiring other companies or assets (Company -> Company/Asset)
- INVESTS_IN: companies investing in projects or companies (Company -> Project/Company)
- RAISES_FUNDS: companies raising funds (Company -> Event, amount in properties)
- REORGANIZES: companies reorganizing (Company -> Event, such as debt restructuring, asset restructuring)
- DISTRIBUTES: companies distributing dividends (Company -> Event, amount in properties)
- HAS_METRIC: companies having financial metric values (Company -> FinancialMetric, value in properties)
- OCCURS_AT: events occurring at locations (Event -> Location)
- COSTS: events/projects involving amounts (Event/Project -> Amount)
- WORKS_AT: people working at companies (Person -> Company)
- LOCATED_IN: companies located at places (Company -> Location)
- SUBSIDIARY_OF: subsidiary relationships (Company -> Company)
- PART_OF: projects belonging to events or plans (Project -> Event)

Important notes:
- Each entity must have a unique and clear name
- Relations must be explicit and meaningful
- Record detailed information in properties, such as time, amount, percentage, etc.
- Only extract entities and relations explicitly mentioned in the document, do not infer

Output in JSON format as follows:
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Company",
            "name": "Entity Name (precise and unique)",
            "properties": {{
                "description": "Entity description (if any)",
                "other_property": "value"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "Relation Type (uppercase English, e.g., HAPPENED_AT)",
            "properties": {{
                "time": "February 2017",
                "amount": "120 million yuan",
                "percentage": "70%",
                "other_property": "value"
            }}
        }}
    ]
}}

Output only JSON, no other text."""


def _generate_law_prompt(text: str, court_name: str, language: str) -> str:
    """ç”Ÿæˆ Law é¢†åŸŸçš„ prompt"""
    if language == "zh":
        return f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ³•å¾‹åˆ¤å†³ä¹¦ä¸­ç²¾ç¡®æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

æ³•é™¢åç§°ï¼š{court_name}

åˆ¤å†³ä¹¦å†…å®¹ï¼š
{text}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼ˆæ³¨æ„ï¼šæ¯ä¸ªå®ä½“å¿…é¡»æœ‰ä¸€ä¸ªå”¯ä¸€ä¸”ç²¾ç¡®çš„åç§°ï¼‰ï¼š

1. **æ³•é™¢å®ä½“** (Court)ï¼š
   - æ³•é™¢åç§°ï¼š{court_name}
   - å…¶ä»–ç›¸å…³æ³•é™¢ã€æ£€å¯Ÿé™¢

2. **æ¡ˆä»¶å®ä½“** (Case)ï¼š
   - æ¡ˆä»¶ç¼–å·ã€æ¡ˆä»¶ç±»å‹ï¼ˆå¦‚äº¤é€šè‚‡äº‹ç½ªã€ç›—çªƒç½ªç­‰ï¼‰

3. **äººç‰©å®ä½“** (Person)ï¼š
   - è¢«å‘Šäººã€åŸå‘Šã€è¾©æŠ¤äººã€æ£€å¯Ÿå®˜ã€æ³•å®˜ã€è¯äºº
   - å…·ä½“å§“åå’Œèº«ä»½

4. **æŒ‡æ§ç½ªåå®ä½“** (Charge)ï¼š
   - å…·ä½“ç½ªåï¼šäº¤é€šè‚‡äº‹ç½ªã€ç›—çªƒç½ªã€è¯ˆéª—ç½ªç­‰

5. **äº‹ä»¶å®ä½“** (Event)ï¼š
   - æ¡ˆä»¶å‘ç”Ÿäº‹ä»¶ï¼šäº¤é€šäº‹æ•…ã€çŠ¯ç½ªè¡Œä¸ºã€è°ƒæŸ¥ç¨‹åºã€åº­å®¡ç¨‹åº
   - ç¨‹åºæ€§äº‹ä»¶ï¼šç«‹æ¡ˆã€æ‹˜ç•™ã€é€®æ•ã€èµ·è¯‰ã€å¼€åº­ã€åˆ¤å†³

6. **æ—¶é—´å®ä½“** (Time)ï¼š
   - å…·ä½“æ—¶é—´ï¼šå¹´ä»½ã€æœˆä»½ã€æ—¥æœŸ
   - æ—¶é—´èŒƒå›´ï¼š2023å¹´4æœˆ20æ—¥ã€2023å¹´5æœˆç­‰

7. **åœ°ç‚¹å®ä½“** (Location)ï¼š
   - æ¡ˆå‘åœ°ç‚¹ï¼šè¡—é“ã€è·¯æ®µã€å…·ä½“åœ°å€
   - ç›¸å…³åœ°ç‚¹ï¼šçœ‹å®ˆæ‰€ã€æ³•é™¢ã€æ£€å¯Ÿé™¢

8. **è¯æ®å®ä½“** (Evidence)ï¼š
   - ç›‘æ§å½•åƒã€è¯äººè¯è¨€ã€æ³•åŒ»é‰´å®šã€ç°åœºç…§ç‰‡ã€äº¤è­¦æŠ¥å‘Šã€åŒ»é™¢è¯Šæ–­ã€é€šè¯è®°å½•ç­‰

9. **åˆ¤å†³ç»“æœå®ä½“** (Verdict)ï¼š
   - åˆ¤å†³å†…å®¹ï¼šæœ‰æœŸå¾’åˆ‘ã€èµ”å¿é‡‘é¢ç­‰

10. **é‡‘é¢å®ä½“** (Amount)ï¼š
    - èµ”å¿é‡‘é¢ã€ç½šæ¬¾é‡‘é¢ç­‰

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å…³ç³»ï¼ˆå…³ç³»ç±»å‹ä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼‰ï¼š

- HAPPENED_AT: äº‹ä»¶å‘ç”Ÿåœ¨æŸä¸ªæ—¶é—´ï¼ˆEvent -> Timeï¼‰
- OCCURS_AT: äº‹ä»¶å‘ç”Ÿåœ¨æŸä¸ªåœ°ç‚¹ï¼ˆEvent -> Locationï¼‰
- INVOLVES: äº‹ä»¶æ¶‰åŠäººç‰©æˆ–å®ä½“ï¼ˆEvent -> Person/Evidence/Locationï¼‰
- CHARGED_WITH: äººç‰©è¢«æŒ‡æ§ç½ªåï¼ˆPerson -> Chargeï¼‰
- DEFENDED_BY: è¢«å‘Šäººç”±è¾©æŠ¤äººè¾©æŠ¤ï¼ˆPerson -> Personï¼‰
- PROSECUTED_BY: æ¡ˆä»¶ç”±æ£€å¯Ÿé™¢èµ·è¯‰ï¼ˆCase -> Court/Personï¼‰
- JUDGED_BY: æ¡ˆä»¶ç”±æ³•é™¢åˆ¤å†³ï¼ˆCase -> Courtï¼‰
- PROVIDES_EVIDENCE: è¯æ®æ”¯æŒæ¡ˆä»¶ï¼ˆEvidence -> Caseï¼‰
- TESTIFIES: è¯äººä½œè¯ï¼ˆPerson -> Case/Eventï¼‰
- RESULTS_IN: æ¡ˆä»¶å¯¼è‡´åˆ¤å†³ç»“æœï¼ˆCase -> Verdictï¼‰
- AWARDS: åˆ¤å†³ç»“æœåŒ…å«èµ”å¿é‡‘é¢ï¼ˆVerdict -> Amountï¼‰
- LOCATED_IN: äººç‰©æˆ–å®ä½“ä½äºæŸä¸ªåœ°ç‚¹ï¼ˆPerson/Location -> Locationï¼‰
- PART_OF: äº‹ä»¶å±äºæ¡ˆä»¶çš„ä¸€éƒ¨åˆ†ï¼ˆEvent -> Caseï¼‰

é‡è¦æç¤ºï¼š
- æ¯ä¸ªå®ä½“å¿…é¡»æœ‰å”¯ä¸€ä¸”æ¸…æ™°çš„åç§°
- å…³ç³»å¿…é¡»æ˜ç¡®ä¸”æœ‰æ„ä¹‰
- åœ¨propertiesä¸­è®°å½•è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æ—¶é—´ã€åœ°ç‚¹ã€é‡‘é¢ç­‰
- åªæå–æ–‡æ¡£ä¸­æ˜ç¡®æåŠçš„å®ä½“å’Œå…³ç³»ï¼Œä¸è¦æ¨æµ‹

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Court",
            "name": "å®ä½“åç§°ï¼ˆç²¾ç¡®ä¸”å”¯ä¸€ï¼‰",
            "properties": {{
                "description": "å®ä½“æè¿°ï¼ˆå¦‚æœ‰ï¼‰",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "å…³ç³»ç±»å‹ï¼ˆä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œå¦‚HAPPENED_ATï¼‰",
            "properties": {{
                "æ—¶é—´": "2023å¹´4æœˆ20æ—¥",
                "åœ°ç‚¹": "è‹¹æœå¸‚ä¸­å¿ƒè¡—é“",
                "é‡‘é¢": "50ä¸‡å…ƒ",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ]
}}

åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    else:
        return f"""You are a knowledge graph construction expert. Extract entities and relations from the following legal judgment document to build a knowledge graph.

Court Name: {court_name}

Judgment Content:
{text}

Please extract the following types of entities (each entity must have a unique and precise name):

1. **Court** entities:
   - Court name: {court_name}
   - Other related courts, procuratorates

2. **Case** entities:
   - Case number, case type (e.g., traffic accident crime, theft crime, etc.)

3. **Person** entities:
   - Defendant, plaintiff, defense attorney, prosecutor, judge, witness
   - Specific names and identities

4. **Charge** entities:
   - Specific charges: traffic accident crime, theft crime, fraud crime, etc.

5. **Event** entities:
   - Case events: traffic accidents, criminal acts, investigation procedures, trial procedures
   - Procedural events: case filing, detention, arrest, prosecution, trial, judgment

6. **Time** entities:
   - Specific times: years, months, dates
   - Time ranges: April 20, 2023, May 2023, etc.

7. **Location** entities:
   - Crime scene: streets, road sections, specific addresses
   - Related locations: detention center, court, procuratorate

8. **Evidence** entities:
   - Surveillance video, witness testimony, forensic identification, scene photos, traffic police reports, hospital diagnosis, call records, etc.

9. **Verdict** entities:
   - Judgment content: fixed-term imprisonment, compensation amount, etc.

10. **Amount** entities:
    - Compensation amount, fine amount, etc.

Please extract the following types of relations (use uppercase English with underscores):

- HAPPENED_AT: events happening at certain times (Event -> Time)
- OCCURS_AT: events occurring at locations (Event -> Location)
- INVOLVES: events involving persons or entities (Event -> Person/Evidence/Location)
- CHARGED_WITH: persons charged with crimes (Person -> Charge)
- DEFENDED_BY: defendant defended by attorney (Person -> Person)
- PROSECUTED_BY: case prosecuted by procuratorate (Case -> Court/Person)
- JUDGED_BY: case judged by court (Case -> Court)
- PROVIDES_EVIDENCE: evidence supporting case (Evidence -> Case)
- TESTIFIES: witness testifying (Person -> Case/Event)
- RESULTS_IN: case resulting in verdict (Case -> Verdict)
- AWARDS: verdict awarding compensation (Verdict -> Amount)
- LOCATED_IN: persons or entities located at places (Person/Location -> Location)
- PART_OF: events being part of case (Event -> Case)

Important notes:
- Each entity must have a unique and clear name
- Relations must be explicit and meaningful
- Record detailed information in properties, such as time, location, amount, etc.
- Only extract entities and relations explicitly mentioned in the document, do not infer

Output in JSON format as follows:
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Court",
            "name": "Entity Name (precise and unique)",
            "properties": {{
                "description": "Entity description (if any)",
                "other_property": "value"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "Relation Type (uppercase English, e.g., HAPPENED_AT)",
            "properties": {{
                "time": "April 20, 2023",
                "location": "City Center Street",
                "amount": "500,000 yuan",
                "other_property": "value"
            }}
        }}
    ]
}}

Output only JSON, no other text."""


def _generate_medical_prompt(text: str, hospital_patient_name: str, language: str) -> str:
    """ç”Ÿæˆ Medical é¢†åŸŸçš„ prompt"""
    if language == "zh":
        return f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹åŒ»ç–—ç—…å†ä¸­ç²¾ç¡®æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

åŒ»é™¢æ‚£è€…åç§°ï¼š{hospital_patient_name}

ç—…å†å†…å®¹ï¼š
{text}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼ˆæ³¨æ„ï¼šæ¯ä¸ªå®ä½“å¿…é¡»æœ‰ä¸€ä¸ªå”¯ä¸€ä¸”ç²¾ç¡®çš„åç§°ï¼‰ï¼š

1. **åŒ»é™¢å®ä½“** (Hospital)ï¼š
   - åŒ»é™¢åç§°ï¼ˆä»hospital_patient_nameä¸­æå–ï¼‰

2. **æ‚£è€…å®ä½“** (Patient)ï¼š
   - æ‚£è€…å§“åï¼ˆä»hospital_patient_nameä¸­æå–ï¼‰
   - æ‚£è€…åŸºæœ¬ä¿¡æ¯ï¼šæ€§åˆ«ã€å¹´é¾„ã€èŒä¸šç­‰

3. **åŒ»ç”Ÿå®ä½“** (Doctor)ï¼š
   - åŒ»ç”Ÿå§“åã€èŒç§°

4. **ç–¾ç—…å®ä½“** (Disease)ï¼š
   - è¯Šæ–­ç–¾ç—…ï¼šåœ°ä¸­æµ·è´«è¡€ã€é«˜è¡€å‹ã€ç³–å°¿ç—…ç­‰

5. **ç—‡çŠ¶å®ä½“** (Symptom)ï¼š
   - ä¸»è¯‰ç—‡çŠ¶ï¼šè´«è¡€ã€ä½“åŠ›ä¸‹é™ã€è…¹éƒ¨çªå‡ºç­‰
   - å…¶ä»–ç—‡çŠ¶ï¼šå¤´æ™•ã€å¿ƒæ‚¸ã€ä¹åŠ›ç­‰

6. **æ£€æŸ¥å®ä½“** (Examination)ï¼š
   - æ£€æŸ¥ç±»å‹ï¼šè¡€å¸¸è§„ã€è…¹éƒ¨è¶…å£°ã€è¡€é“è›‹ç™½æµ‹å®šã€è¡€ç”ŸåŒ–æ£€æŸ¥ç­‰

7. **æ£€æŸ¥ç»“æœå®ä½“** (ExaminationResult)ï¼š
   - æ£€æŸ¥ç»“æœæ•°å€¼ï¼šè¡€çº¢è›‹ç™½80g/Lã€çº¢ç»†èƒè®¡æ•°3.2x10^12/Lç­‰

8. **æ²»ç–—å®ä½“** (Treatment)ï¼š
   - æ²»ç–—æ–¹æ³•ï¼šé“è¯åˆå‰‚æ²»ç–—ã€è¯ç‰©æ²»ç–—ç­‰

9. **è¯ç‰©å®ä½“** (Medication)ï¼š
   - è¯ç‰©åç§°ï¼šé“è¯åˆå‰‚ç­‰

10. **æ—¶é—´å®ä½“** (Time)ï¼š
    - å…¥é™¢æ—¶é—´ã€è®°å½•æ—¶é—´ã€æ£€æŸ¥æ—¶é—´ç­‰

11. **åœ°ç‚¹å®ä½“** (Location)ï¼š
    - æ‚£è€…ä½å€ã€åŒ»é™¢åœ°å€ç­‰

12. **è¯Šæ–­å®ä½“** (Diagnosis)ï¼š
    - åˆæ­¥è¯Šæ–­ã€é‰´åˆ«è¯Šæ–­ç­‰

è¯·æå–ä»¥ä¸‹ç±»å‹çš„å…³ç³»ï¼ˆå…³ç³»ç±»å‹ä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼‰ï¼š

- ADMITTED_TO: æ‚£è€…å…¥é™¢ï¼ˆPatient -> Hospitalï¼‰
- TREATED_BY: æ‚£è€…ç”±åŒ»ç”Ÿæ²»ç–—ï¼ˆPatient -> Doctorï¼‰
- HAS_SYMPTOM: æ‚£è€…æœ‰ç—‡çŠ¶ï¼ˆPatient -> Symptomï¼‰
- DIAGNOSED_WITH: æ‚£è€…è¢«è¯Šæ–­ä¸ºç–¾ç—…ï¼ˆPatient -> Diseaseï¼‰
- UNDERGOES: æ‚£è€…æ¥å—æ£€æŸ¥ï¼ˆPatient -> Examinationï¼‰
- SHOWS_RESULT: æ£€æŸ¥æ˜¾ç¤ºç»“æœï¼ˆExamination -> ExaminationResultï¼‰
- INDICATES: æ£€æŸ¥ç»“æœæŒ‡ç¤ºç–¾ç—…ï¼ˆExaminationResult -> Diseaseï¼‰
- RECEIVES_TREATMENT: æ‚£è€…æ¥å—æ²»ç–—ï¼ˆPatient -> Treatmentï¼‰
- USES_MEDICATION: æ²»ç–—ä½¿ç”¨è¯ç‰©ï¼ˆTreatment -> Medicationï¼‰
- PRESCRIBED_BY: è¯ç‰©ç”±åŒ»ç”Ÿå¼€å…·ï¼ˆMedication -> Doctorï¼‰
- HAPPENED_AT: äº‹ä»¶å‘ç”Ÿåœ¨æŸä¸ªæ—¶é—´ï¼ˆEvent -> Timeï¼‰
- LOCATED_IN: æ‚£è€…æˆ–åŒ»é™¢ä½äºæŸä¸ªåœ°ç‚¹ï¼ˆPatient/Hospital -> Locationï¼‰
- PART_OF: ç—‡çŠ¶æˆ–æ£€æŸ¥å±äºè¯Šæ–­çš„ä¸€éƒ¨åˆ†ï¼ˆSymptom/Examination -> Diagnosisï¼‰

é‡è¦æç¤ºï¼š
- æ¯ä¸ªå®ä½“å¿…é¡»æœ‰å”¯ä¸€ä¸”æ¸…æ™°çš„åç§°
- å…³ç³»å¿…é¡»æ˜ç¡®ä¸”æœ‰æ„ä¹‰
- åœ¨propertiesä¸­è®°å½•è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æ—¶é—´ã€æ•°å€¼ã€å•ä½ç­‰
- åªæå–æ–‡æ¡£ä¸­æ˜ç¡®æåŠçš„å®ä½“å’Œå…³ç³»ï¼Œä¸è¦æ¨æµ‹

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Patient",
            "name": "å®ä½“åç§°ï¼ˆç²¾ç¡®ä¸”å”¯ä¸€ï¼‰",
            "properties": {{
                "description": "å®ä½“æè¿°ï¼ˆå¦‚æœ‰ï¼‰",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "å…³ç³»ç±»å‹ï¼ˆä½¿ç”¨è‹±æ–‡å¤§å†™ï¼Œå¦‚HAS_SYMPTOMï¼‰",
            "properties": {{
                "æ—¶é—´": "2æœˆ4æ—¥",
                "æ•°å€¼": "80g/L",
                "å…¶ä»–å±æ€§": "å€¼"
            }}
        }}
    ]
}}

åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    else:
        return f"""You are a knowledge graph construction expert. Extract entities and relations from the following medical record to build a knowledge graph.

Hospital Patient Name: {hospital_patient_name}

Medical Record Content:
{text}

Please extract the following types of entities (each entity must have a unique and precise name):

1. **Hospital** entities:
   - Hospital name (extracted from hospital_patient_name)

2. **Patient** entities:
   - Patient name (extracted from hospital_patient_name)
   - Patient basic information: gender, age, occupation, etc.

3. **Doctor** entities:
   - Doctor name, title

4. **Disease** entities:
   - Diagnosed diseases: thalassemia, hypertension, diabetes, etc.

5. **Symptom** entities:
   - Chief complaints: anemia, physical decline, abdominal protrusion, etc.
   - Other symptoms: dizziness, palpitations, fatigue, etc.

6. **Examination** entities:
   - Examination types: blood routine, abdominal ultrasound, ferritin test, blood biochemistry, etc.

7. **ExaminationResult** entities:
   - Examination result values: hemoglobin 80g/L, red blood cell count 3.2x10^12/L, etc.

8. **Treatment** entities:
   - Treatment methods: iron chelator therapy, medication, etc.

9. **Medication** entities:
   - Medication names: iron chelator, etc.

10. **Time** entities:
    - Admission time, record time, examination time, etc.

11. **Location** entities:
    - Patient address, hospital address, etc.

12. **Diagnosis** entities:
    - Preliminary diagnosis, differential diagnosis, etc.

Please extract the following types of relations (use uppercase English with underscores):

- ADMITTED_TO: patient admitted to hospital (Patient -> Hospital)
- TREATED_BY: patient treated by doctor (Patient -> Doctor)
- HAS_SYMPTOM: patient has symptom (Patient -> Symptom)
- DIAGNOSED_WITH: patient diagnosed with disease (Patient -> Disease)
- UNDERGOES: patient undergoes examination (Patient -> Examination)
- SHOWS_RESULT: examination shows result (Examination -> ExaminationResult)
- INDICATES: examination result indicates disease (ExaminationResult -> Disease)
- RECEIVES_TREATMENT: patient receives treatment (Patient -> Treatment)
- USES_MEDICATION: treatment uses medication (Treatment -> Medication)
- PRESCRIBED_BY: medication prescribed by doctor (Medication -> Doctor)
- HAPPENED_AT: events happening at certain times (Event -> Time)
- LOCATED_IN: patients or hospitals located at places (Patient/Hospital -> Location)
- PART_OF: symptoms or examinations being part of diagnosis (Symptom/Examination -> Diagnosis)

Important notes:
- Each entity must have a unique and clear name
- Relations must be explicit and meaningful
- Record detailed information in properties, such as time, values, units, etc.
- Only extract entities and relations explicitly mentioned in the document, do not infer

Output in JSON format as follows:
{{
    "entities": [
        {{
            "id": "e1",
            "type": "Patient",
            "name": "Entity Name (precise and unique)",
            "properties": {{
                "description": "Entity description (if any)",
                "other_property": "value"
            }}
        }}
    ],
    "relations": [
        {{
            "source": "e1",
            "target": "e2",
            "type": "Relation Type (uppercase English, e.g., HAS_SYMPTOM)",
            "properties": {{
                "time": "February 4",
                "value": "80g/L",
                "other_property": "value"
            }}
        }}
    ]
}}

Output only JSON, no other text."""


# =========================
# åˆä½µå¤šæ–‡ä»¶çš„ KG çµæœ
# =========================
def merge_knowledge_graphs(kg_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆå¹¶å¤šä¸ªæ–‡æ¡£çš„çŸ¥è¯†å›¾è°±æå–ç»“æœ

    è¿”å›æ ¼å¼ï¼š
    {
        "entities": [...],   # å»é‡å¾Œçš„å¯¦é«”
        "relations": [...],  # å·²æ›´æ–° source/target ç‚ºå…¨å±€ idï¼Œä¸¦å¸¶ doc_id
        "triples": [...],    # â­ æ–¹ä¾¿æª¢ç´¢çš„ä¸‰å…ƒçµ„åˆ—è¡¨ (head, relation, tail, doc_id, ...)
        "doc_mapping": {...},
        "statistics": {...}
    }
    """
    all_entities: List[Dict[str, Any]] = []
    all_relations: List[Dict[str, Any]] = []

    # key: "type:name" -> entityï¼ˆç”¨ä¾†åšå»é‡ï¼‰
    entity_map: Dict[str, Dict[str, Any]] = {}
    # åŸå§‹ per-doc entity id -> å…¨å±€ entity idï¼ˆe1, e2, ...ï¼‰
    entity_id_mapping: Dict[str, str] = {}
    next_entity_id = 1

    # doc_id -> è©² doc ç‰½æ¶‰åˆ°çš„å…¨å±€ entity ids / relation indices
    doc_mapping: Dict[int, Dict[str, Any]] = {}

    for kg_result in kg_results:
        doc_id = kg_result.get("doc_id")
        if doc_id is None:
            continue

        doc_entity_ids: List[str] = []
        doc_relation_ids: List[int] = []

        # === è™•ç†å¯¦é«”ï¼ˆå«å»é‡èˆ‡å±¬æ€§åˆä½µï¼‰ ===
        for entity in kg_result.get("entities", []):
            entity_name = (entity.get("name") or "").strip()
            entity_type = entity.get("type", "")
            original_id = entity.get("id", "")

            if not entity_name:
                continue

            # æ¨™æº–åŒ–åç¨±
            normalized_name = " ".join(entity_name.split())
            if entity_type in ["Time", "Amount", "FinancialMetric"]:
                # å°æ™‚é–“ã€é‡‘é¡ç­‰ï¼Œä¸ç‰¹åˆ¥å£“ç©ºç™½ï¼Œé¿å…å¤±çœŸ
                normalized_name = entity_name.strip()

            key = f"{entity_type}:{normalized_name}"

            if key not in entity_map:
                # æ–°å¯¦é«”
                new_id = f"e{next_entity_id}"
                next_entity_id += 1
                entity["id"] = new_id
                entity["name"] = normalized_name
                entity_map[key] = entity
                all_entities.append(entity)

                if original_id:
                    entity_id_mapping[original_id] = new_id
                final_id = new_id
            else:
                # å·²å­˜åœ¨ï¼Œåˆä½µå±¬æ€§
                existing_entity = entity_map[key]
                existing_props = existing_entity.get("properties", {}) or {}
                new_props = entity.get("properties", {}) or {}

                # description ç‰¹åˆ¥è™•ç†ï¼šç›¡é‡æ‹¼èµ·ä¾†ï¼Œä¸è¦è¦†è“‹
                if "description" in new_props and "description" in existing_props:
                    if new_props["description"] not in existing_props["description"]:
                        existing_props["description"] += f"; {new_props['description']}"
                elif "description" in new_props:
                    existing_props["description"] = new_props["description"]

                # å…¶ä»–å±¬æ€§ï¼šç°¡å–® mergeï¼ˆæ–°å€¼è£œåˆ°èˆŠå€¼è£¡ï¼‰
                for k, v in new_props.items():
                    if k == "description":
                        continue
                    if k not in existing_props:
                        existing_props[k] = v

                existing_entity["properties"] = existing_props

                if original_id:
                    entity_id_mapping[original_id] = existing_entity["id"]
                final_id = existing_entity["id"]

            if final_id not in doc_entity_ids:
                doc_entity_ids.append(final_id)

        # === è™•ç†é—œä¿‚ï¼ˆæ›´æ–° source/target IDï¼Œä¸¦ç°¡å–®å»é‡ï¼‰ ===
        # æ³¨æ„ï¼šexisting_relation_keys å¿…é ˆåœ¨ã€Œæ‰€æœ‰ doc å…±ç”¨ã€ï¼Œæ‰€ä»¥æ”¾åœ¨å¤–é¢
        # ä½†é€™è£¡è¦å…ˆç¢ºä¿å®ƒå·²ç¶“å­˜åœ¨æ–¼å¤–å±¤ scope
        # æˆ‘å€‘å¯ä»¥åœ¨é€™è£¡åˆ¤æ–·ï¼Œå¦‚æœ all_relations æ˜¯ç©ºçš„æ‰åˆå§‹åŒ–ï¼Œé¿å…æ¯æ¬¡ loop é‡å»º
        if not hasattr(merge_knowledge_graphs, "_relation_keys"):
            merge_knowledge_graphs._relation_keys = set()
        existing_relation_keys = merge_knowledge_graphs._relation_keys  # type: ignore

        for relation in kg_result.get("relations", []):
            source_old = relation.get("source", "")
            target_old = relation.get("target", "")

            # æŠŠåŸå§‹ per-doc entity id æ˜ å°„åˆ°å…¨å±€ id
            source_new = entity_id_mapping.get(source_old, source_old)
            target_new = entity_id_mapping.get(target_old, target_old)

            relation_obj = {
                "source": source_new,
                "target": target_new,
                "type": relation.get("type", ""),
                "properties": relation.get("properties", {}) or {},
                "doc_id": doc_id,
            }

            relation_key = (relation_obj["source"], relation_obj["target"], relation_obj["type"], doc_id)
            if relation_key not in existing_relation_keys:
                all_relations.append(relation_obj)
                existing_relation_keys.add(relation_key)
                doc_relation_ids.append(len(all_relations) - 1)

        doc_mapping[doc_id] = {
            "entities": doc_entity_ids,
            "relations": doc_relation_ids,
        }

    # === ç”Ÿæˆä¸‰å…ƒçµ„ triple list (head, relation, tail, doc_id, ...) ===
    id_to_entity = {e["id"]: e for e in all_entities if "id" in e}
    triples: List[Dict[str, Any]] = []

    for rel in all_relations:
        src_id = rel.get("source")
        tgt_id = rel.get("target")
        rel_type = rel.get("type", "")
        doc_id = rel.get("doc_id")

        head_ent = id_to_entity.get(src_id, {})
        tail_ent = id_to_entity.get(tgt_id, {})

        triples.append(
            {
                "head": head_ent.get("name", src_id),
                "head_id": src_id,
                "head_type": head_ent.get("type", ""),
                "relation": rel_type,
                "tail": tail_ent.get("name", tgt_id),
                "tail_id": tgt_id,
                "tail_type": tail_ent.get("type", ""),
                "doc_id": doc_id,
                "properties": rel.get("properties", {}) or {},
            }
        )

    return {
        "entities": all_entities,
        "relations": all_relations,
        "triples": triples,  # â­ çµ¦ retriever ç”¨çš„ headâ€“relationâ€“tailâ€“doc_id ä¸‰å…ƒçµ„
        "doc_mapping": doc_mapping,
        "statistics": {
            "total_entities": len(all_entities),
            "total_relations": len(all_relations),
            "total_triples": len(triples),
            "total_docs": len(doc_mapping),
        },
    }

# =========================
# Checkpoint è®€å¯«
# =========================

def load_checkpoint(checkpoint_path: str) -> Dict[str, Any]:
    """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    if Path(checkpoint_path).exists():
        try:
            with open(checkpoint_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if "processed_doc_ids" in data and isinstance(data["processed_doc_ids"], list):
                    data["processed_doc_ids"] = set(data["processed_doc_ids"])
                return data
        except Exception as e:
            print(f"Warning: Failed to load checkpoint: {e}")
            return {"processed_doc_ids": set(), "kg_results": []}
    return {"processed_doc_ids": set(), "kg_results": []}


def save_checkpoint(checkpoint_path: str, processed_doc_ids: Set[int], kg_results: List[Dict[str, Any]]):
    """ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)
    checkpoint_data = {
        "processed_doc_ids": list(processed_doc_ids),
        "kg_results": kg_results,
    }
    with open(checkpoint_path, "w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)


def save_partial_kg(output_path: str, kg_results: List[Dict[str, Any]]):
    """ä¿å­˜éƒ¨åˆ†çŸ¥è¯†å›¾è°±"""
    if not kg_results:
        return

    merged_kg = merge_knowledge_graphs(kg_results)
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(merged_kg, f, ensure_ascii=False, indent=2)

    print(
        f"ğŸ’¾ Saved partial KG: {len(kg_results)} docs, "
        f"{merged_kg['statistics']['total_entities']} entities, "
        f"{merged_kg['statistics']['total_relations']} relations"
    )


# =========================
# ä¸»æµç¨‹ï¼šå¾æ–‡æª”å»º KG
# =========================
def build_kg(
    docs_path: str,
    output_path: str,
    language: str = "zh",
    batch_size: int = 1,      # å…ˆä¿ç•™ï¼Œä¸ç‰¹åˆ¥ç”¨
    limit: int = None,
    save_interval: int = 20,  # å»ºè­°ç•¥å¾®èª¿å¤§ï¼Œé¿å…å¤ªé »ç¹ I/O
    resume: bool = True,
    max_workers: int = 4,     # â­ ä½µç™¼æ•¸ï¼šå¯ä»¥å…ˆç”¨ 3~5ï¼Œä¹‹å¾Œå†èª¿
):
    """
    ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ”¯æ´å¢é‡ä¿å­˜ã€æ–­ç‚¹ç»­ä¼ ã€ä½µå‘è™•ç†ï¼‰

    Args:
        docs_path: è¾“å…¥æ–‡æ¡£è·¯å¾„ (JSONL)
        output_path: è¾“å‡ºçŸ¥è¯†å›¾è°±è·¯å¾„ (JSON)
        language: è¯­è¨€ ("zh" æˆ– "en")
        batch_size: ä¿ç•™åƒæ•¸ï¼Œç›®å‰æœªä½¿ç”¨
        limit: é™åˆ¶å¤„ç†çš„æ–‡æ¡£æ•°é‡
        save_interval: æ¯å¤„ç†å¤šå°‘ä¸ªæ–‡æ¡£ä¿å­˜ä¸€æ¬¡
        resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        max_workers: ä½µç™¼ worker æ•¸é‡ï¼ˆå»ºè­° 3~5ï¼Œé¿å… API è¢«æ‰“çˆ†ï¼‰
    """
    print(f"Loading documents from {docs_path}...")
    docs = load_jsonl(docs_path)
    print(f"Loaded {len(docs)} documents.")

    if limit is not None and limit > 0:
        docs = docs[:limit]
        print(f"âš ï¸  TEST MODE: Processing only first {len(docs)} documents.")

    checkpoint_path = str(Path(output_path).with_suffix(".checkpoint.json"))
    kg_results: List[Dict[str, Any]] = []
    processed_doc_ids: Set[int] = set()

    # ==== 1. å¾ checkpoint æ¢å¾© ====
    if resume and Path(checkpoint_path).exists():
        print(f"ğŸ“‚ Found checkpoint file: {checkpoint_path}")
        checkpoint = load_checkpoint(checkpoint_path)
        processed_doc_ids_raw = checkpoint.get("processed_doc_ids", set())
        if isinstance(processed_doc_ids_raw, list):
            processed_doc_ids = set(processed_doc_ids_raw)
        else:
            processed_doc_ids = processed_doc_ids_raw
        kg_results = checkpoint.get("kg_results", [])
        print(f"   Resuming from checkpoint: {len(processed_doc_ids)} docs already processed")

        if kg_results:
            save_partial_kg(output_path, kg_results)

    # æŠŠé‚„æ²’è™•ç†éçš„ doc æŠ½å‡ºä¾†
    docs_to_process = [doc for doc in docs if doc.get("doc_id") not in processed_doc_ids]
    total_to_process = len(docs_to_process)
    print(f"Total docs to process this run: {total_to_process}")

    if total_to_process == 0:
        print("No new documents to process. Merging existing KG results...")
        merged_kg = merge_knowledge_graphs(kg_results)
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(merged_kg, f, ensure_ascii=False, indent=2)
        print("Done (no new docs).")
        return merged_kg

    print(f"Extracting entities and relations with up to {max_workers} parallel workers...")
    print(f"   Save interval: every {save_interval} newly processed documents")

    newly_processed_count = 0

    # ==== 2. ä½µç™¼åŸ·è¡Œæ¯ç¯‡æ–‡ä»¶çš„æŠ½å– ====
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # å»ºç«‹ future -> doc å°æ‡‰ï¼Œæ–¹ä¾¿ debug
        future_to_docid = {}
        for doc in docs_to_process:
            future = executor.submit(_process_single_doc, doc, language)
            future_to_docid[future] = doc.get("doc_id")

        # ç”¨ tqdm åŒ…è£ as_completedï¼Œé¡¯ç¤ºé€²åº¦
        for future in tqdm(as_completed(future_to_docid), total=total_to_process, desc="Processing docs (parallel)"):
            doc_id, kg_result, error_msg = future.result()

            # æœ‰äº› doc å¯èƒ½ doc_id æ˜¯ None æˆ– content ç©º
            if doc_id is None:
                continue

            if error_msg is not None:
                print(f"âŒ Error processing doc_id {doc_id}: {error_msg}")
                # å³ä½¿éŒ¯èª¤ä¹Ÿè¨˜éŒ„ç‚ºå·²è™•ç†ï¼Œé¿å…ç„¡é™é‡è©¦
                processed_doc_ids.add(doc_id)
                newly_processed_count += 1
            else:
                if kg_result is not None:
                    kg_results.append(kg_result)
                processed_doc_ids.add(doc_id)
                newly_processed_count += 1

            # å®šæœŸä¿å­˜ partial KG + checkpoint
            if newly_processed_count > 0 and newly_processed_count % save_interval == 0:
                save_partial_kg(output_path, kg_results)
                save_checkpoint(checkpoint_path, processed_doc_ids, kg_results)
                print(
                    f"   Progress: {newly_processed_count}/{total_to_process} "
                    f"new docs processed in this run (total processed: {len(processed_doc_ids)})"
                )

    # ==== 3. æœ€çµ‚åˆä½µèˆ‡ä¿å­˜ ====
    print("Merging knowledge graphs...")
    merged_kg = merge_knowledge_graphs(kg_results)

    print("Knowledge graph built successfully!")
    print(f"  - Total entities: {merged_kg['statistics']['total_entities']}")
    print(f"  - Total relations: {merged_kg['statistics']['total_relations']}")
    print(f"  - Total documents: {merged_kg['statistics']['total_docs']}")

    print(f"Saving final knowledge graph to {output_path}...")
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(merged_kg, f, ensure_ascii=False, indent=2)

    if Path(checkpoint_path).exists():
        Path(checkpoint_path).unlink()
        print("âœ… Checkpoint file removed (processing complete)")

    print("Done!")
    return merged_kg

# =========================
# CLI å…¥å£
# =========================

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Build Knowledge Graph from dragonball_docs.jsonl")
    parser.add_argument(
        "--input",
        type=str,
        default="dragonball_dataset/dragonball_docs.jsonl",
        help="Input documents path (JSONL)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="My_RAG/kg_output.json",
        help="Output knowledge graph path (JSON)",
    )
    parser.add_argument(
        "--language",
        type=str,
        default="zh",
        choices=["zh", "en"],
        help="Language (zh or en)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit the number of documents to process (for testing, e.g., --limit 5)",
    )
    parser.add_argument(
        "--save-interval",
        type=int,
        default=10,
        help="Save progress every N documents (default: 10)",
    )
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="Do not resume from checkpoint (start from scratch)",
    )

    args = parser.parse_args()

    # è½‰æˆå°ˆæ¡ˆæ ¹ç›®éŒ„çš„ç›¸å°è·¯å¾‘
    root_dir = Path(__file__).parent.parent
    input_path = root_dir / args.input
    output_path = root_dir / args.output

    build_kg(
        docs_path=str(input_path),
        output_path=str(output_path),
        language=args.language,
        limit=args.limit,
        save_interval=args.save_interval,
        resume=not args.no_resume,
    )


if __name__ == "__main__":
    main()
